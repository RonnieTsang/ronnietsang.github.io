<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hive | RonnieTsang's Blog]]></title>
  <link href="http://RonnieTsang.github.io/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://RonnieTsang.github.io/"/>
  <updated>2014-09-12T17:33:21+08:00</updated>
  <id>http://RonnieTsang.github.io/</id>
  <author>
    <name><![CDATA[RonnieTsang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Writing a Hive Generic UDF]]></title>
    <link href="http://RonnieTsang.github.io/blog/2014/09/05/writing-a-hive-generic-udf/"/>
    <updated>2014-09-05T05:40:22+08:00</updated>
    <id>http://RonnieTsang.github.io/blog/2014/09/05/writing-a-hive-generic-udf</id>
    <content type="html"><![CDATA[<h2>业务背景</h2>

<p>公司的数据分析业务 ETL 过程中，原始数据经过数据清洗之后，统一转换成自定义的 protobuf <code>CustomEventMessage</code> 格式，用户自定义事件 ( 多个KV对的格式 ) 保存在 <code>event_args</code> 字段中。最终 <code>CustomEventMessage</code> 数据将导入 <code>hive</code> 中，在没有 <code>Protobuf Serde</code> 之前，这需要先将数据重新转换为文本格式。
最后，在 <code>hive</code> 的建表语句中 <code>event_args</code> 字段声明为 <code>MAP</code> 类型：</p>

<pre><code class="sql">    event_args MAP&lt;INT, INT&gt;
or
    event_args MAP&lt;STRING, STRING&gt;
</code></pre>

<p>举例来说，某个 <code>event_args</code> 字段带有两个自定义参数 <code>money</code> 和 <code>paytype</code>，转换后的文本内容为：（ Key 1 和 3 是预定的 ）</p>

<pre><code class="sql">    1:200|3:1
or
    money:200|paytype:1
</code></pre>

<p>在后续 hive 查询脚本中引用该字段提取其中特定的事件（ 比如支付类型 <code>paytype</code> ）很简单且直观：</p>

<pre><code class="sql">    event_args[3]
or
    event_args['paytype']
</code></pre>

<h3>存在的问题及对策</h3>

<p>多了转文本这一步骤，维护起来太繁琐，因此开发了 <strong>ProtobufSerde</strong> ，让 <code>hive</code> 能直接支持存取 <code>protobuf</code> 格式的数据，这样 <code>CustomEventMessage</code> 入库 hive 将会方便很多。</p>

<p>当然，现在的 hive 程序也需要做一些相应的调整，比如上述 <code>event_args</code> 字段的读取就麻烦得多了。现在的建表语句变成如下形式（直接对应 proto）：</p>

<pre><code class="sql">event_args ARRAY&lt; STRUCT&lt;key_int:BIGINT,key_str:STRING,value_int:BIGINT,value_str:STRING&gt; &gt;
</code></pre>

<p><code>select</code> 得到的数据格式将是类似这样的：</p>

<pre><code class="sql">[{"key_int":1,"key_str":"money","value_int":200,"value_str":"200"},{"key_int":3,"key_str":"paytype","value_int":1,"value_str":"1"}]
</code></pre>

<p>在这里例子中，为了得到 <code>paytype</code>，我们可以在查询语句中这样书写：</p>

<pre><code class="sql">SELECT event_args[1].value_int FROM s_CustomEventMessage WHERE pyearmonth='201403' and pday='07' and event_id=30;
</code></pre>

<h3>引入的新问题</h3>

<p>看起来依旧方便的这种写法，基于 <code>2</code> 个前提：</p>

<blockquote><ul>
<li><code>event_args</code> 中有 <code>paytype</code> 事件</li>
<li><code>paytype</code> 自定义事件在 <code>event_args</code> 中是第 <code>2</code> 个 item（ index 1 ）</li>
</ul>
</blockquote>

<p>遗憾的是，这两个条件都没有办法满足。因此现在需要通过 遍历整个 <code>event_args</code> 的内容，找出我们所关心的事件。
OK，很明显，我们需要 <code>UDF</code> 来完成这一任务。</p>

<hr />

<h2>如何编写 Hive GenericUDF</h2>

<p>为编写 UDF，Hive 中提供了两套不同的接口，分别是：</p>

<blockquote><ul>
<li>Simple API - <code>org.apache.hadoop.hive.ql.exec.UDF</code></li>
<li>Complex API - <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</code></li>
</ul>
</blockquote>

<p><code>UDF</code> 接口非常容易编写，但其局限性在于它只适用于函数输入参数和返回值都是 <code>Java原生类型</code> 的场景，这里我们的输入参数类型是 <code>ARRAY&lt; STRUCT&lt;key_int:BIGINT,key_str:STRING,value_int:BIGINT,value_str:STRING&gt; &gt;</code>，返回值是 <code>BIGINT</code> 或者 <code>STRING</code> 类型，因此 <code>UDF</code> 接口满足不了需求。</p>

<p>GenericUDF 是 Hive UDFs 中的 <strong>瑞士军刀</strong>，适用处理 Hive 的复杂类型如 <code>MAP</code>、<code>ARRAY</code>、<code>STRUCT</code> 以及在此之上的各种嵌套类型。下面我们就使用它来完成这一任务。</p>

<h3>几点准备知识：</h3>

<blockquote><ul>
<li>UDF 函数通过 <code>ObjectInspector</code> 来访问传入的参数，因此 <code>ObjectInspector</code> 的概念很重要
（对于 ObjectInspector 不太懂，等了解学习以后单独写个文档介绍）</li>
<li>GenericUDFs 中使用的很多函数，特别是 <code>ObjectInspector</code> 的众多子类函数，返回 <code>Object</code> 类型，这意味着：
（1） 编译器无法帮助我们做类型检查
（2） 很多时候我们不知道拿到手的对象的实际类型（hadoop 的 tasklogs 是很有用的工具，可以借助它来调试）</li>
</ul>
</blockquote>

<p>所有的自定义 <code>GenericUDFs</code> 继承自 <code>GenericUDF</code> 类，因此需要实现以下三个函数：</p>

<pre><code class="java">public ObjectInspector initialize(ObjectInspector[] arguments)
public Object evaluate(DeferredObject[] arguments)
public String getDisplayString(String[] children)
</code></pre>

<h3>1） initialize()函数</h3>

<p>initialize() 函数在 UDF 第一次调用时执行，主要完成四件事：</p>

<blockquote><ul>
<li>检查输入参数类型（在我们的例子中，即 <code>array&lt;struct&lt;key_int:bigint,key_str:string,value_int:bigint,value_str:string&gt;&gt;</code> 和 <code>string</code>）</li>
<li>设置并返回一个跟函数返回值匹配的 <code>ObjectInspector</code> 对象</li>
<li>把输入参数中用到的各种 <code>ObjectInspectors</code> 保存到全局变量中</li>
<li>设置返回值</li>
</ul>
</blockquote>

<h3>2）evaluate() 函数</h3>

<p>evaluate() 函数接收输入实参，完成计算后返回结果。如前所述，我们通过各种 <code>ObjectInspectors</code> 来访问传入的参数，<code>ObjectInspectors</code> 是由我们在 initialize() 函数中保存起来的。我们需要知道：</p>

<blockquote><ul>
<li>array&lt;> 在 Java 中用 ArrayList&lt;> 来表示</li>
<li>struct&lt;> 在 Java 中用 Object[] 来表示</li>
<li>使用 hadoop 的 Writable 和 Text 类而非 Java 原生类型</li>
</ul>
</blockquote>

<h3>3） getDisplayString() 函数</h3>

<p>不那么重要，我们只需返回一个字符串，表示该 UDF 是干什么的就行了，在我们使用 <code>EXPLAIN</code> 调试 <code>hql</code> 语句时，该信息会被输出。</p>

<hr />

<h2>代码实现</h2>

<p>代码中已经有了详尽的注释来解释每一个步骤，可以参考着上面的内容来理解。不再详述。</p>

<pre><code class="java">package com.mjyun.hive.udf;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;

import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;

import org.apache.hadoop.hive.serde2.objectinspector.StructField;

import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;

import org.apache.hadoop.hive.serde2.lazy.LazyString;
import org.apache.hadoop.hive.serde2.lazy.LazyLong;

import java.util.ArrayList;


public class UDFGetValueFromEventArgsStr extends GenericUDF {

    // Global variables that inspect the input.
    // These are set up during the initialize() call, and are then used during the calls to evaluate()
    private ListObjectInspector   listOI;                   // ObjectInspector for the list (array&lt;&gt;)
    private StructObjectInspector structOI;                 // ObjectInspector for the struct&lt;&gt;
    private ObjectInspector       kiOI, ksOI, viOI, vsOI;   // ObjectInspector for the elements of the struct&lt;&gt;: key_int, key_str, value_int, value_str
    private StringObjectInspector targetOI;                 // ObjectInspector for the second arg that we match with target field

    public String getDisplayString ( String []arg0 ) {
        return "getValueFromEventArgsStr";
    }

    // this is like the evaluate method of the simple API. 
    // It takes the actual arguments and returns the result.
    public ObjectInspector initialize ( ObjectInspector[] arguments ) throws UDFArgumentException {

        if ( arguments.length != 2 ) {
            throw new UDFArgumentTypeException(0, "getValueFromEventArgsStr only takes 2 arguments: List&lt;Struct&lt;bigint,string,bigint,string&gt;&gt;, String");
        }

        // 1. Check we received the right object types.
        ObjectInspector a = arguments[0];
        ObjectInspector b = arguments[1];
        if ( !( a instanceof ListObjectInspector ) ) {
            throw new UDFArgumentTypeException(0, "The first argument must be a Array&lt;Struct&gt;, "
                    + " but " + a.getTypeName() + " is found");
        }
        if ( !( b instanceof StringObjectInspector ) ) {
            throw new UDFArgumentTypeException(0, "The second argument must be a String, "
                    + " but " + b.getTypeName() + " is found");
        }

        // 2. Get the object inspector for the list(array) elements; this should be a StructObjectInspector
        // Then check that the struct has the correct fields.
        // Also, store the ObjectInspectors for use later in the evaluate() method
        listOI   = (ListObjectInspector)a;
        targetOI = (StringObjectInspector)b;
        structOI = (StructObjectInspector)listOI.getListElementObjectInspector();

        if ( structOI.getAllStructFieldRefs().size() != 4 ) {
            throw new UDFArgumentTypeException(0, "Incorrect number of fields in the struct. "
                    + "The first argument of type Array&lt;Struct&gt; must contains 4 fields, "
                    + " but " + structOI.getAllStructFieldRefs().size() + " fields is found");
        }

        StructField key_int   = structOI.getStructFieldRef("key_int");
        StructField key_str   = structOI.getStructFieldRef("key_str");
        StructField value_int = structOI.getStructFieldRef("value_int");
        StructField value_str = structOI.getStructFieldRef("value_str");

        if ( null == key_int )
            throw new UDFArgumentTypeException(0, "No \"key_int\" field in input structure " + arguments[0].getTypeName());
        if ( null == key_str )
            throw new UDFArgumentTypeException(0, "No \"key_str\" field in input structure " + arguments[0].getTypeName());
        if ( null == value_int )
            throw new UDFArgumentTypeException(0, "No \"value_int\" field in input structure " + arguments[0].getTypeName());
        if ( null == value_str )
            throw new UDFArgumentTypeException(0, "No \"value_str\" field in input structure " + arguments[0].getTypeName());

        kiOI = key_int.getFieldObjectInspector();
        ksOI = key_str.getFieldObjectInspector();
        viOI = value_int.getFieldObjectInspector();
        vsOI = value_str.getFieldObjectInspector();

        if ( kiOI.getCategory() != ObjectInspector.Category.PRIMITIVE )
            throw new UDFArgumentTypeException(0, "Is input primitive? key_int field must be a bigint, but " + kiOI.getTypeName() + " is found");
        if ( ksOI.getCategory() != ObjectInspector.Category.PRIMITIVE )
            throw new UDFArgumentTypeException(0, "Is input primitive? key_str field must be a string, but " + ksOI.getTypeName() + " is found");
        if ( viOI.getCategory() != ObjectInspector.Category.PRIMITIVE )
            throw new UDFArgumentTypeException(0, "Is input primitive? value_int field must be a bigint, but " + viOI.getTypeName() + " is found");
        if ( vsOI.getCategory() != ObjectInspector.Category.PRIMITIVE )
            throw new UDFArgumentTypeException(0, "Is input primitive? value_str field must be a string, but " + vsOI.getTypeName() + " is found");

        if ( ((PrimitiveObjectInspector)kiOI).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.LONG )
            throw new UDFArgumentTypeException(0, "Is input correct primitive? key_int field must be a bigint, but " + kiOI.getTypeName() + " is found");
        if ( ((PrimitiveObjectInspector)ksOI).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING )
            throw new UDFArgumentTypeException(0, "Is input correct primitive? key_str field must be a string, but " + ksOI.getTypeName() + " is found");
        if ( ((PrimitiveObjectInspector)viOI).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.LONG )
            throw new UDFArgumentTypeException(0, "Is input correct primitive? value_int field must be a bigint, but " + viOI.getTypeName() + " is found");
        if ( ((PrimitiveObjectInspector)vsOI).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING )
            throw new UDFArgumentTypeException(0, "Is input correct primitive? value_str field must be a bigint, but " + vsOI.getTypeName() + " is found");

        // 3. the return type of our function is a bigint, so we provide the correct object inspector
        return PrimitiveObjectInspectorFactory.javaLongObjectInspector;
    }

    // The evaluate() method. The input is passed in as an array of DeferredObjects, so that
    // computation is not wasted on deserializing them if they're not actually used
    public Object evaluate( DeferredObject[] arguments ) throws HiveException {

        // Should take 2 arguments
        if ( arguments.length != 2 )
            return null;

        if ( null == arguments[0].get() )
            return null;

        // Iterate over the elements of the input array
        // If a key_str of arguments[1] if found, return key_int
        // Else return null
        int nelements = listOI.getListLength(arguments[0].get()), val_int;
        for ( int i = 0; i &lt; nelements; i++ ) {
            Text tt = (Text)(structOI.getStructFieldData( listOI.getListElement(arguments[0].get(), i), structOI.getStructFieldRef("key_str") ));
            String t = tt.toString();
            // String skey_str = targetOI.getPrimitiveJavaObject(LSkey_str);
            String target = targetOI.getPrimitiveJavaObject(arguments[1].get());
            if ( t.equals(target) ) {
                Long ll = ((LongWritable)(structOI.getStructFieldData( listOI.getListElement(arguments[0].get(), i), structOI.getStructFieldRef("value_int") ))).get();
                return ll;
            }
        }

        return null;
    }    

}
</code></pre>

<hr />

<h2>小结</h2>

<p>本文粗浅地介绍了一个处理 <code>hive</code> <code>array&lt;struct&lt;&gt;&gt;</code> 数据类型的 <code>GenericUDF</code> 应该如何编写，大概就是这些了，第一次接触，略作记录，仅供参考。</p>

<hr />

<h2>参考</h2>

<ol>
<li><a href="http://hive.apache.org/javadocs/r0.10.0/api/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.html">官方GenericUDF API</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">HivePlugins</a></li>
<li><a href="https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-12/ch12-section-08">User-Defined Functions</a></li>
<li><a href="http://www.congiu.com/structured-data-in-hive-a-generic-udf-to-sort-arrays-of-structs/">Structured data in Hive: a generic UDF to sort arrays of structs</a></li>
<li><a href="http://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html">Hadoop Hive UDF Tutorial - Extending Hive with Custom Functions</a></li>
<li><a href="http://www.baynote.com/2012/11/a-word-from-the-engineers/">Writing a Hive Generic UDF</a></li>
</ol>

]]></content>
  </entry>
  
</feed>
